{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting a path to cinematic success\n",
    "#### Milestone 2 - TheArmadaOfBadassEmpanadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we embark on a journey to craft a data pre-processing pipeline, essential for the success of our project. Our exploration centers around the CMU Movie Corpus dataset, with a keen focus on developing two key components: a plot summary data frame and a movie metadata data frame.\n",
    "\n",
    "Guided by the narrative of our project, we aim to unearth initial insights through a meticulous process of data cleaning, reformating, and enrichment:\n",
    "\n",
    "- **Data Cleaning:** This involves identifying and rectifying inaccuracies or inconsistencies in our dataset. It includes tasks like handling missing values, removing duplicates, and correcting data entry errors. This process ensures the reliability and quality of our data for accurate analysis.\n",
    "\n",
    "- **Reformatting into Database-Like Structure:** We will restructure the data into a more organized, database-like format. This step includes categorizing data into tables, establishing relational links, and indexing for efficient querying and retrieval. This structured approach enhances data accessibility and usability.\n",
    "\n",
    "- **Fetching Information from Wikipedia API:** To enrich our dataset, we will integrate external data by fetching relevant information from the Wikipedia API. \n",
    "\n",
    "After we have cleaned, reformatted, and enhanced our data, we will explore various dimensions like box office revenues, representation of women in film, and sentiment analysis. Our exploration aims to identify correlations among different variables and glean meaningful insights. Ultimately, this process is designed to address the key questions outlined in our project's storytelling narrative, enabling us to draw significant conclusions from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#--- General imports\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "#--- Imports for data vizualization\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#--- Imports for data cleaning\n",
    "import pywikibot\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "#--- Imports for queries on Wikidata\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "#--- Imports for summary extraction\n",
    "import gzip\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source: http://www.cs.cmu.edu/~ark/personas/\n",
    "\n",
    "# Define the paths\n",
    "DATA_PATH = 'data/'\n",
    "CLEAN_DATA_PATH = 'clean_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 1: Data cleaning \n",
    "\n",
    "The objective here is to clean and standardize the dataset. This involves removing JSON strings and unifying the data format. Additionally, countries in the dataset will be categorized into global regions for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we already cleaned the data before submission, we can skip those steps\n",
    "PERFORM_CLEANING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Grouping the countries into world regions\n",
    "\n",
    "Initially, the plan was to classify movies into Hollywood, Bollywood, and Nollywood categories. However, due to a lack of sufficient Nollywood data and the absence of this categorization in the dataset, this approach was modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movies data\n",
    "movies_df = pd.read_csv(DATA_PATH + 'movie.metadata.tsv', delimiter='\\t', header=None, names=['wiki_id', 'id', 'name', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres'])\n",
    "\n",
    "# Convert the json string to a list of countries\n",
    "movies_df[\"countries\"] = movies_df[\"countries\"].apply(lambda x: list(json.loads(x).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countries\n",
       "United States of America    34408\n",
       "India                        8411\n",
       "United Kingdom               7868\n",
       "France                       4395\n",
       "Italy                        3163\n",
       "Japan                        2647\n",
       "Canada                       2534\n",
       "Germany                      2393\n",
       "Argentina                    1468\n",
       "Hong Kong                    1240\n",
       "Spain                        1136\n",
       "Australia                    1114\n",
       "South Korea                   887\n",
       "Mexico                        870\n",
       "Netherlands                   840\n",
       "Sweden                        657\n",
       "West Germany                  647\n",
       "China                         645\n",
       "Denmark                       610\n",
       "Soviet Union                  564\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of movies that are assigned to each countries. \n",
    "# Duplicate the movies if they have multiple countries.\n",
    "\n",
    "nb_movies_per_country = movies_df[\"countries\"].explode().value_counts()\n",
    "display(nb_movies_per_country.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the 20 first most represented country can be used for Nollywood. Since there also will be data cleaning in the next steps due to NaN information. We decided to not use this categorization.\n",
    "\n",
    "Instead, movies will be grouped based on the following global regions:\n",
    "- North America: United States of America, Canada\n",
    "- Europe: United Kingdom, France, Italy, Germany, Spain, and others\n",
    "- Asia: Japan, Hong Kong, China, South Korea, Taiwan\n",
    "- India: India, Pakistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countries\n",
       "Northen America    36942\n",
       "Europe             21571\n",
       "Other              13731\n",
       "India               8600\n",
       "Asia                5595\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "areas = {\n",
    "\t\"Northen America\": set([\"United States of America\", \"Canada\"]),\n",
    "\t\"Europe\": set([\"United Kingdom\", \"France\", \"Italy\", \"Germany\", \"Spain\", \"West Germany\", \"Belgium\", \"German Democratic Republic\", \"Ireland\", \"Switzerland\", \"Austria\", \"England\", \"Luxembourg\", \"Portugal\"]),\n",
    "\t\"Asia\": set([\"Japan\", \"Hong Kong\", \"China\", \"South Korea\", \"Taiwan\"]),\n",
    "\t\"India\": set([\"India\", \"Pakistan\"])\n",
    "}\n",
    "country_to_area = {country: area for area, countries in areas.items() for country in countries}\n",
    "\n",
    "def get_area(country):\n",
    "\tif country in country_to_area:\n",
    "\t\treturn country_to_area[country]\n",
    "\telse:\n",
    "\t\treturn \"Other\"\n",
    "\n",
    "# Count the number of movies that are assigned to each area.\n",
    "# Duplicate the movies if they have multiple countries.\n",
    "movies_areas_df = movies_df.copy()\n",
    "movies_areas_df = movies_areas_df['countries'].apply(lambda x: [get_area(country) for country in x])\n",
    "nb_movies_per_area = movies_areas_df.explode().value_counts()\n",
    "display(nb_movies_per_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, those four categories will have enough data for us to still be able to clean them and do the analysis.\n",
    "\n",
    "Keep in mind that those categories are not perfect, and that some countries are missing, or overlap. That's why we now will group the countries into those categories.\n",
    "\n",
    "First, one thing that stroke us is that each movie can have multiple countries. The question is how to deal with that ? In fact most of the movies have not much countries, but some have a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bar plot of the number of countries per movie (log scale)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Number of countries per movie\")\n",
    "plt.xlabel(\"Number of countries\")\n",
    "plt.ylabel(\"Number of movies\")\n",
    "sns.countplot(x=movies_df[\"countries\"].apply(len))\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we saw, is that 'United States of America' are frequently part of the countries of a movie. \n",
    "One way to see it is to plot a heatmap of the countries of a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_copy_df = movies_df.copy()\n",
    "# Only keep the countries that are in the areas\n",
    "movies_copy_df[\"countries\"] = movies_copy_df[\"countries\"].apply(lambda x: [country for country in x if country in country_to_area])\n",
    "# Only keep the movies that have at least one country\n",
    "movies_copy_df = movies_copy_df[movies_copy_df[\"countries\"].apply(len) > 0]\n",
    "# Only keep the movies that have countries that are not in the same area\n",
    "movies_copy_df[\"areas\"] = movies_copy_df[\"countries\"].apply(lambda x: set([country_to_area[country] for country in x]))\n",
    "movies_copy_df = movies_copy_df[movies_copy_df[\"areas\"].apply(lambda x: len(x) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a heatmap of the combinations of countries\n",
    "countries_df = pd.DataFrame(columns=country_to_area.keys(), index=country_to_area.keys())\n",
    "countries_df = countries_df.fillna(0)\n",
    "for index, row in movies_copy_df.iterrows():\n",
    "\tcountries_list = list(row[\"countries\"])\n",
    "\tpairs = combinations(countries_list, 2)\n",
    "\tfor pair in pairs:\n",
    "\t\tcountries_df[pair[0]][pair[1]] += 1\n",
    "\t\tcountries_df[pair[1]][pair[0]] += 1\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(countries_df, annot=True, cmap=\"YlGnBu\", fmt='g', cbar_kws={'label': 'Count'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a heatmap of the combination of areas\n",
    "areas_df = pd.DataFrame(columns=areas.keys(), index=areas.keys())\n",
    "areas_df = areas_df.fillna(0)\n",
    "for index, row in movies_copy_df.iterrows():\n",
    "\tareas_list = list(row[\"areas\"])\n",
    "\tpairs = list(combinations(areas_list, 2))\n",
    "\tfor pair in pairs:\n",
    "\t\tareas_df[pair[0]][pair[1]] += 1\n",
    "\t\tareas_df[pair[1]][pair[0]] += 1\n",
    "\n",
    "sns.heatmap(areas_df, annot=True, cmap=\"YlGnBu\", fmt='g', cbar_kws={'label': 'Count'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: What can we conclude about these heatmaps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since nothing is specified in the dataset about what represent those countries, we will assume that we cannot choose when they are not in the same region.\n",
    "\n",
    "One way to handle this problem is to say that if there is only one country in the list, we will use it. If there are multiple countries and every of those countries are in the same world region, we will group them in the region directly. If there are multiple countries and they are not all in the same region, we will use the Wikipedia API to get the real country of the movie.\n",
    "\n",
    "In fact, this third option involving GET requests takes a lot of time (less than 2 movies per second), so we wanted to avoid using it as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default area to None\n",
    "movies_df[\"area\"] = None\n",
    "\n",
    "print(f\"There are {len(movies_df)} movies in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the areas if the number of countries played in is 1\n",
    "movies_without_area_df = movies_df[movies_df[\"countries\"].apply(lambda x: len(x) == 1)]\n",
    "movies_df.loc[movies_without_area_df.index, \"area\"] = movies_without_area_df[\"countries\"].apply(lambda x: get_area(x[0]))\n",
    "\n",
    "print(f\"Fetching areas for {len(movies_without_area_df)} movies (remaining {len(movies_df[movies_df['area'].isnull()])})\")\n",
    "display(movies_df.loc[movies_without_area_df.index, [\"id\", \"countries\", \"area\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_from_unanimity(countries):\n",
    "\tarea = None\n",
    "\tfor country in countries:\n",
    "\t\tif country in country_to_area:\n",
    "\t\t\tif area is None:\n",
    "\t\t\t\tarea = country_to_area[country]\n",
    "\t\t\telif area != country_to_area[country]:\n",
    "\t\t\t\treturn None\n",
    "\t\telse:\n",
    "\t\t\treturn None\n",
    "\treturn area\n",
    "\n",
    "# For each remaining movie (only the ones with None as area), if the countries played in are in the same area, set the area to that area\n",
    "movies_without_area_df = movies_df[movies_df[\"area\"].isnull()]\n",
    "movies_df.loc[movies_without_area_df.index, \"area\"] = movies_without_area_df[\"countries\"].apply(lambda x: get_area_from_unanimity(x))\n",
    "\n",
    "print(f\"Combining areas for {len(movies_without_area_df)} movies (remaining {len(movies_df[movies_df['area'].isnull()])})\")\n",
    "display(movies_df.loc[movies_without_area_df.index, [\"id\", \"countries\", \"area\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step took us more than 8 hours to run, we decided to save the data in a csv file, and to load it directly in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = pywikibot.Site('en', 'wikipedia')\n",
    "country_property = 'P495'\n",
    "\n",
    "def get_area_from_wikipedia(movie_wiki_id):\n",
    "\ttry:\n",
    "\t\twiki_api_url = f'https://en.wikipedia.org/w/api.php?action=query&pageids={movie_wiki_id}&format=json'\n",
    "\t\tjson_data = requests.get(wiki_api_url).json()\n",
    "\t\tmovie_title = json_data[\"query\"][\"pages\"][str(movie_wiki_id)][\"title\"]\n",
    "\t\tmovie_page = pywikibot.Page(wikipedia, movie_title)\n",
    "\t\tmovie_item = pywikibot.ItemPage.fromPage(movie_page)\n",
    "\t\tmovie_item_dict = movie_item.get()\n",
    "\t\tmovie_country = movie_item_dict[\"claims\"][country_property][0].getTarget().labels[\"en\"]\n",
    "\t\treturn country_to_area[movie_country] if movie_country in country_to_area else \"Other\"\n",
    "\texcept:\n",
    "\t\treturn \"Not found\"\n",
    "\t\n",
    "# For each remaining movie (that had the number of countries played in != 1), we set the area retrieved from wikipedia by scraping\n",
    "movies_without_area_df = movies_df[movies_df[\"area\"].isnull()]\n",
    "if PERFORM_CLEANING:\n",
    "\tmovies_df.loc[movies_without_area_df.index, \"area\"] = movies_without_area_df[\"wiki_id\"].apply(lambda x: get_area_from_wikipedia(x))\n",
    "\n",
    "print(f\"Retriving areas for {len(movies_without_area_df)} movies (remaining {len(movies_df[movies_df['area'].isnull()])})\")\n",
    "display(movies_df.loc[movies_without_area_df.index, [\"id\", \"countries\", \"area\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the most part of the movies has a country and therefore a world region attached to it. It will then be used during the rest of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cleaning and reformat the metadata\n",
    "\n",
    "We will now clean the data and reformat it. We will do the following steps:\n",
    "- Remove the json strings\n",
    "- Add a column that only contains the year of the movie\n",
    "- Separate the multiple values into other dataframes (to have a database like structure)\n",
    "- Save them in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the json columns for the movies table\n",
    "\n",
    "# Separate the languages to another table\n",
    "movies_df[\"languages\"] = movies_df[\"languages\"].apply(lambda x: list(json.loads(x).values())) # Convert the json string to a list of languages\n",
    "movies_languages_df = movies_df[[\"id\", \"languages\"]].explode(\"languages\").reset_index(drop=True)\n",
    "movies_languages_df = movies_languages_df.rename(columns={\"languages\": \"language\", \"id\": \"movie_id\"})\n",
    "display(movies_languages_df.head())\n",
    "\n",
    "# Separate the genres to another table\n",
    "movies_df[\"genres\"] = movies_df[\"genres\"].apply(lambda x: list(json.loads(x).values())) # Convert the json string to a list of genres\n",
    "movies_genres_df = movies_df[[\"id\", \"genres\"]].explode(\"genres\").reset_index(drop=True)\n",
    "movies_genres_df = movies_genres_df.rename(columns={\"genres\": \"genre\", \"id\": \"movie_id\"})\n",
    "display(movies_genres_df.head())\n",
    "\n",
    "# Separate the countries to another table\n",
    "# The countries are already a list of countries\n",
    "movies_countries_playedin_df = movies_df[[\"id\", \"countries\"]].explode(\"countries\").reset_index(drop=True)\n",
    "movies_countries_playedin_df = movies_countries_playedin_df.rename(columns={\"countries\": \"country\", \"id\": \"movie_id\"})\n",
    "display(movies_countries_playedin_df.head())\n",
    "\n",
    "# Drop the useless columns (that have been separated)\n",
    "movies_df = movies_df.drop(columns=[\"languages\", \"genres\", \"countries\"])\n",
    "display(movies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get year from formats 'YYYY-MM-DD' or 'YYYY'\n",
    "def get_year(date: str):\n",
    "\ttry:\n",
    "\t\tyear = datetime.datetime.strptime(date, '%Y-%m-%d').year\n",
    "\t\treturn year\n",
    "\texcept:\n",
    "\t\tyear = date\n",
    "\t\treturn year\n",
    "\n",
    "# Add the column release_year to the movies table\n",
    "movies_df[\"release_year\"] = movies_df[\"release_date\"].apply(lambda x: get_year(x))\n",
    "display(movies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the summaries of the movies in a separate table, using the movie_id instead of the wiki_id\n",
    "# We could have directly merged the summaries with the movies table, but it is a very large text field\n",
    "\n",
    "# Load the summaries\n",
    "movies_summaries_df = pd.read_csv(DATA_PATH + 'plot_summaries.txt', delimiter='\\t', header=None, names=['wiki_id', 'summary'])\n",
    "display(movies_summaries_df.head())\n",
    "\n",
    "# Merge the summaries with the movies table, to change the wiki_id to movie_id\n",
    "movies_summaries_df = movies_summaries_df.merge(movies_df[[\"wiki_id\", \"id\"]], on=\"wiki_id\")\n",
    "movies_summaries_df = movies_summaries_df.drop(columns=[\"wiki_id\"])\n",
    "movies_summaries_df = movies_summaries_df.rename(columns={\"id\": \"movie_id\"})\n",
    "display(movies_summaries_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the characters in a separate table\n",
    "\n",
    "# Load the characters\n",
    "characters_df = pd.read_csv(DATA_PATH + 'character.metadata.tsv', delimiter='\\t', header=None, names=['movie_wiki_id', 'movie_id', 'movie_release_date', 'name', 'actor_birth_date', 'actor_gender', 'actor_height','actor_ethinicity_id', 'actor_name', 'actor_age', 'actor_map_id', 'id', 'actor_id'])\n",
    "# Add the column release_year to the character table\n",
    "characters_df[\"movie_release_year\"] = characters_df[\"movie_release_date\"].apply(lambda x: get_year(x))\n",
    "display(characters_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the characters categories table\n",
    "\n",
    "# Load the characters categories\n",
    "characters_categories_df = pd.read_csv(DATA_PATH + 'tvtropes.clusters.txt', delimiter='\\t', header=None, names=['category', 'meta'])\n",
    "display(characters_categories_df.head())\n",
    "\n",
    "# Fetch the characters ids from the meta column, and remove the other information that are redundant with the characters table\n",
    "characters_categories_df[\"meta\"] = characters_categories_df[\"meta\"].apply(lambda x: json.loads(x))\n",
    "characters_categories_df[\"character_id\"] = characters_categories_df[\"meta\"].apply(lambda x: x[\"id\"])\n",
    "characters_categories_df = characters_categories_df.drop(columns=[\"meta\"])\n",
    "display(characters_categories_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to save everything in a csv file, so that we can load it in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERFORM_CLEANING:\n",
    "\t# Make sure to create the clean_data folder before running this script\n",
    "\t\n",
    "\t# Remove all the previous files from the clean_data folder\n",
    "\tfor filename in os.listdir(CLEAN_DATA_PATH):\n",
    "\t\tos.remove(CLEAN_DATA_PATH + filename)\n",
    "\n",
    "\t# Save the data\n",
    "\tmovies_df.to_csv(CLEAN_DATA_PATH + 'movies.csv', index=False)\n",
    "\tmovies_languages_df.to_csv(CLEAN_DATA_PATH + 'movies_languages.csv', index=False)\n",
    "\tmovies_genres_df.to_csv(CLEAN_DATA_PATH + 'movies_genres.csv', index=False)\n",
    "\tmovies_countries_playedin_df.to_csv(CLEAN_DATA_PATH + 'movies_countries_playedin.csv', index=False)\n",
    "\tmovies_summaries_df.to_csv(CLEAN_DATA_PATH + 'movies_summaries.csv', index=False)\n",
    "\tcharacters_df.to_csv(CLEAN_DATA_PATH + 'characters.csv', index=False)\n",
    "\tcharacters_categories_df.to_csv(CLEAN_DATA_PATH + 'characters_categories.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what we did, here is a little overview of the tables related to the movies:\n",
    "- movies.csv\n",
    "- movies_countries_playedin.csv\n",
    "- movies_genres.csv\n",
    "- movies_languages.csv\n",
    "- movies_summaries.csv\n",
    "\n",
    "To use all of those tables (except movies), we will need to merge them to the movie table (in a database like structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(CLEAN_DATA_PATH + 'movies.csv')\n",
    "display(movies_df.head())\n",
    "\n",
    "# Add the language to the movies table\n",
    "languages_df = pd.read_csv(CLEAN_DATA_PATH + 'movies_languages.csv')\n",
    "movies_df = pd.merge(movies_df, languages_df, left_on=\"id\", right_on=\"movie_id\", how=\"left\")\n",
    "display(movies_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the languages can be multiple for a single movie, so the joined table will have multiple rows for a single movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Plot summaries extraction and cleaning\n",
    "The corenlp_plot_summaries dataset is composed of gzipped folders, each containing a single xml file with matching name. Each folder-file corresponds to one movie, and the name is the movie ID.\n",
    "\n",
    "We thus need to extract all zipped folder, and get the XML file. To do this, we will follow these steps:\n",
    "1. Extract the gzipped folder using gzip package.\n",
    "2. Move extracted files to a unique folder in clean_data.\n",
    "3. Remove the empty folders after moving the files.\n",
    "\n",
    "Same as above, this step only need to be performed one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the zipped folders\n",
    "NLP_DATA_PATH = '../corenlp_plot_summaries/'\n",
    "\n",
    "# Path to the new data folder\n",
    "NLP_SUMMARIES = 'clean_data/NLP_summaries/'\n",
    "\n",
    "# Create the new data folder if it doesn't exist\n",
    "if not os.path.exists(NLP_SUMMARIES):\n",
    "    os.makedirs(NLP_SUMMARIES)\n",
    "\n",
    "# Iterate through each gzipped folder\n",
    "for root, dirs, files in os.walk(NLP_DATA_PATH):\n",
    "    for file in files:\n",
    "        # Check if the file is a gzipped file\n",
    "        if file.endswith('.gz'):\n",
    "            # Path to the gzipped file contained in the folder\n",
    "            gz_path = os.path.join(root, file)\n",
    "\n",
    "            # Create a folder name based on the gzipped file name, removing the extension\n",
    "            folder_name = os.path.splitext(file)[0]\n",
    "\n",
    "            # Path to the destination folder\n",
    "            destination_folder = os.path.join(NLP_SUMMARIES, folder_name)\n",
    "\n",
    "            # Unzip the contents of the gzipped file to the destination folder\n",
    "            with gzip.open(gz_path, 'rb') as gz_file:\n",
    "                with open(destination_folder, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(gz_file, out_file)\n",
    "\n",
    "# Move all XML files from the unzipped folders to the clean data folder\n",
    "for root, dirs, files in os.walk(NLP_SUMMARIES):\n",
    "    for file in files:\n",
    "        # Check if the file is an XML file\n",
    "        if file.endswith('.xml'):\n",
    "            # Path to the XML file\n",
    "            xml_path = os.path.join(root, file)\n",
    "\n",
    "            # Move the XML file\n",
    "            shutil.move(xml_path, os.path.join(NLP_SUMMARIES, file))\n",
    "\n",
    "# Remove the empty folders left after moving the XML files\n",
    "for root, dirs, files in os.walk(NLP_SUMMARIES, topdown=False):\n",
    "    for folder in dirs:\n",
    "        folder_path = os.path.join(root, folder)\n",
    "        os.rmdir(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each XML file is stored in clean_data folder and can be read to analyze summary informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data exploration and visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groundwork is now in place for easy data accessibility, given our planned analysis. It's time to dive deeper into our data. Our initial focus is dual: evaluating data integrity, particularly regarding missing data, and gaining an overview of its characteristics. We will sequentially assess the following  parameters—film box-office revenue, movie genre, actor gender, and actor ethnicity — than we plan to integrate within our data story. This stage serves as a critical precursor, guiding us to identify parameters suitable for exploration in the next milestone and determining the direction that our next analysis will take. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Exploration of the box-office revenue of movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need separate the data and create individuals dataframe from the movies dataframe for each geographical region previously defined, using the area feature added before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_id</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>area</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24229100</td>\n",
       "      <td>/m/07kjkz6</td>\n",
       "      <td>Lady Snowblood 2: Love Song of Vengeance</td>\n",
       "      <td>1974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.0</td>\n",
       "      <td>Asia</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>26878691</td>\n",
       "      <td>/m/0f400r</td>\n",
       "      <td>Mysterious Island</td>\n",
       "      <td>1982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Asia</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>13281430</td>\n",
       "      <td>/m/03c0d85</td>\n",
       "      <td>My Name is Fame</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "      <td>Asia</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>24196090</td>\n",
       "      <td>/m/07k8x7p</td>\n",
       "      <td>The World of Geisha</td>\n",
       "      <td>1973-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Asia</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>17780234</td>\n",
       "      <td>/m/0479c20</td>\n",
       "      <td>Banana Club</td>\n",
       "      <td>1996-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asia</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      wiki_id          id                                      name  \\\n",
       "11   24229100  /m/07kjkz6  Lady Snowblood 2: Love Song of Vengeance   \n",
       "43   26878691   /m/0f400r                         Mysterious Island   \n",
       "56   13281430  /m/03c0d85                           My Name is Fame   \n",
       "108  24196090  /m/07k8x7p                       The World of Geisha   \n",
       "114  17780234  /m/0479c20                               Banana Club   \n",
       "\n",
       "    release_date  revenue  runtime  area release_year  \n",
       "11          1974      NaN     89.0  Asia         1974  \n",
       "43          1982      NaN    100.0  Asia         1982  \n",
       "56          2006      NaN     94.0  Asia         2006  \n",
       "108   1973-11-03      NaN     73.0  Asia         1973  \n",
       "114   1996-04-13      NaN      NaN  Asia         1996  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_id</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>area</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9363483</td>\n",
       "      <td>/m/0285_cd</td>\n",
       "      <td>White Of The Eye</td>\n",
       "      <td>1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261236</td>\n",
       "      <td>/m/01mrr1</td>\n",
       "      <td>A Woman in Flames</td>\n",
       "      <td>1983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2345652</td>\n",
       "      <td>/m/075f66</td>\n",
       "      <td>City of the Dead</td>\n",
       "      <td>1960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>171005</td>\n",
       "      <td>/m/016ywb</td>\n",
       "      <td>Henry V</td>\n",
       "      <td>1989-11-08</td>\n",
       "      <td>10161099.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30388930</td>\n",
       "      <td>/m/0g5qvzg</td>\n",
       "      <td>1919</td>\n",
       "      <td>1984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wiki_id          id               name release_date     revenue  runtime  \\\n",
       "3    9363483  /m/0285_cd   White Of The Eye         1987         NaN    110.0   \n",
       "4     261236   /m/01mrr1  A Woman in Flames         1983         NaN    106.0   \n",
       "9    2345652   /m/075f66   City of the Dead         1960         NaN     76.0   \n",
       "13    171005   /m/016ywb            Henry V   1989-11-08  10161099.0    137.0   \n",
       "16  30388930  /m/0g5qvzg               1919         1984         NaN     99.0   \n",
       "\n",
       "      area release_year  \n",
       "3   Europe         1987  \n",
       "4   Europe         1983  \n",
       "9   Europe         1960  \n",
       "13  Europe         1989  \n",
       "16  Europe         1984  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_id</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>area</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20604092</td>\n",
       "      <td>/m/0523t_1</td>\n",
       "      <td>Anbu Thozhi</td>\n",
       "      <td>2007-08-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11448183</td>\n",
       "      <td>/m/02rc_h4</td>\n",
       "      <td>Bindiya Chamkegi</td>\n",
       "      <td>1984-01-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9633533</td>\n",
       "      <td>/m/02pml15</td>\n",
       "      <td>Vandanam</td>\n",
       "      <td>1989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>India</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>29528534</td>\n",
       "      <td>/m/04j0lfk</td>\n",
       "      <td>Anokha Rishta</td>\n",
       "      <td>1986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>India</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>29772142</td>\n",
       "      <td>/m/0fq260_</td>\n",
       "      <td>Karayilekku Oru Kadal Dooram</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>India</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wiki_id          id                          name release_date  revenue  \\\n",
       "27  20604092  /m/0523t_1                   Anbu Thozhi   2007-08-17      NaN   \n",
       "30  11448183  /m/02rc_h4              Bindiya Chamkegi   1984-01-20      NaN   \n",
       "31   9633533  /m/02pml15                      Vandanam         1989      NaN   \n",
       "35  29528534  /m/04j0lfk                 Anokha Rishta         1986      NaN   \n",
       "37  29772142  /m/0fq260_  Karayilekku Oru Kadal Dooram   2010-12-31      NaN   \n",
       "\n",
       "    runtime   area release_year  \n",
       "27      NaN  India         2007  \n",
       "30      NaN  India         1984  \n",
       "31    168.0  India         1989  \n",
       "35    180.0  India         1986  \n",
       "37    110.0  India         2010  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_id</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>area</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>14010832.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Northen America</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3196793</td>\n",
       "      <td>/m/08yl5d</td>\n",
       "      <td>Getting Away with Murder: The JonBenét Ramsey ...</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Northen America</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13696889</td>\n",
       "      <td>/m/03cfc81</td>\n",
       "      <td>The Gangsters</td>\n",
       "      <td>1913-05-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Northen America</td>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10408933</td>\n",
       "      <td>/m/02qc0j7</td>\n",
       "      <td>Alexander's Ragtime Band</td>\n",
       "      <td>1938-08-16</td>\n",
       "      <td>3600000.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>Northen America</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>175026</td>\n",
       "      <td>/m/017n1p</td>\n",
       "      <td>Sarah and Son</td>\n",
       "      <td>1930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>Northen America</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wiki_id          id                                               name  \\\n",
       "0     975900   /m/03vyhn                                     Ghosts of Mars   \n",
       "1    3196793   /m/08yl5d  Getting Away with Murder: The JonBenét Ramsey ...   \n",
       "5   13696889  /m/03cfc81                                      The Gangsters   \n",
       "7   10408933  /m/02qc0j7                           Alexander's Ragtime Band   \n",
       "10    175026   /m/017n1p                                      Sarah and Son   \n",
       "\n",
       "   release_date     revenue  runtime             area release_year  \n",
       "0    2001-08-24  14010832.0     98.0  Northen America         2001  \n",
       "1    2000-02-16         NaN     95.0  Northen America         2000  \n",
       "5    1913-05-29         NaN     35.0  Northen America         1913  \n",
       "7    1938-08-16   3600000.0    106.0  Northen America         1938  \n",
       "10         1930         NaN     86.0  Northen America         1930  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the data\n",
    "movies_df = pd.read_csv(CLEAN_DATA_PATH + 'movies.csv')\n",
    "\n",
    "#Create a separate dataframe with data of each area\n",
    "movies_revenue_df=movies_df.copy()\n",
    "asia_movies_df = movies_revenue_df[movies_revenue_df[\"area\"] == \"Asia\"]\n",
    "europe_movies_df = movies_revenue_df[movies_revenue_df[\"area\"] == \"Europe\"]\n",
    "india_movies_df = movies_revenue_df[movies_revenue_df[\"area\"] == \"India\"]\n",
    "northen_america_movies_df = movies_revenue_df[movies_revenue_df[\"area\"] == \"Northen America\"]\n",
    "\n",
    "display(asia_movies_df.head())\n",
    "display(europe_movies_df.head())\n",
    "display(india_movies_df.head())\n",
    "display(northen_america_movies_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing any analysis on this feature, we need to check if there are some missing box-office revenue data. If so, we need to determine to which extent, in order to decide the best way to handle it, and determine how it could impact our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the proportion of missing revenue data\n",
    "def percentage_missing_data(df, region, ax):\n",
    "    # Indicator variable showing that something is NA\n",
    "    df.loc[:,[\"revenue_is_na\"]] = df.revenue.isna()\n",
    "\n",
    "    # % of missing data\n",
    "    missing_revenue_data = df.groupby(\"release_year\").revenue_is_na.mean()\n",
    "    \n",
    "    # Plot it\n",
    "    ax.plot(missing_revenue_data)\n",
    "    ax.set(xlabel='Year', ylabel='Percentage')\n",
    "    ax.set_title(region)\n",
    "    x_values = ax.get_xticks()\n",
    "    new_x = [x_values[i] for i in [0, len(x_values) //2, -1]]\n",
    "    ax.set_xticks(new_x)\n",
    "    \n",
    "    #return overall percentage of missing data\n",
    "    prop_missing_data=sum(df[\"revenue_is_na\"])*100/len(df)\n",
    "    return prop_missing_data\n",
    "\n",
    "fig, axs = plt.subplots(4, sharey=True, figsize=(15, 10), layout='constrained')\n",
    "fig.suptitle(\"Percentage of missing data per year\")\n",
    "prop_missing_asia=percentage_missing_data(asia_movies_df, \"Asia\", axs[0])\n",
    "prop_missing_europe=percentage_missing_data(europe_movies_df, \"Europe\", axs[1])\n",
    "prop_missing_india=percentage_missing_data(india_movies_df, \"India\", axs[2])\n",
    "prop_missing_northen_america=percentage_missing_data(northen_america_movies_df, \"Northen America\", axs[3])\n",
    "plt.show()\n",
    "\n",
    "# PrintOverall proportion of missing data\n",
    "print(\"Percentage of missing revenue data:\")\n",
    "print(\"- Asia: %.2f\" % prop_missing_asia)\n",
    "print(\"- Europe: %.2f\" % prop_missing_europe)\n",
    "print(\"- India: %.2f\" % prop_missing_india)\n",
    "print(\"- Northen America: %.2f\" % prop_missing_northen_america)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the data exhibits a lot of missing values, ranging from 82.19% to 99.67% across various regions. Particularly noteworthy is the near 100% absence of revenue data for India, as well as substantial gaps over extended timeframes in the other regions. Consequently, conducting a meaningful analysis becomes unfeasible. The dataset lacks uniformity in revenue representation among areas, and represents only select years. As a result, we've opted against analyzing film revenue and instead shifted focus to other parameters that offer richer insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2 Exploration of the movie genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "genres_df = pd.read_csv(CLEAN_DATA_PATH + 'movies_genres.csv')\n",
    "display(genres_df.head())\n",
    "\n",
    "#Merge movie genre to movie dataset\n",
    "movies_genres_df = pd.merge(movies_df, genres_df, left_on=\"id\", right_on=\"movie_id\", how=\"left\")\n",
    "display(movies_genres_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Similarly to the movie revenue, we separate the data by creating individuals dataframe from the movies dataframe for each geographical region previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asia_genres_df = movies_genres_df[movies_genres_df[\"area\"] == \"Asia\"]\n",
    "display(asia_genres_df.head())\n",
    "europe_genres_df = movies_genres_df[movies_genres_df[\"area\"] == \"Europe\"]\n",
    "display(europe_genres_df.head())\n",
    "india_genres_df = movies_genres_df[movies_genres_df[\"area\"] == \"India\"]\n",
    "display(india_genres_df.head())\n",
    "northen_america_genres_df = movies_genres_df[movies_genres_df[\"area\"] == \"Northen America\"]\n",
    "display(northen_america_genres_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the revenue, before performing any analysis on this caracteristic, we need to determine the proportion of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute the proportion of missing data about movie genre\n",
    "def percentage_missing_data(df, region, ax):\n",
    "    # Indicator variable showing that something is NA\n",
    "    df.loc[:,[\"genre_is_na\"]] = df.genre.isna()\n",
    "\n",
    "    # % of missing data\n",
    "    missing_genre_data = df.groupby(\"release_year\").genre_is_na.mean()\n",
    "    \n",
    "    # Plot it\n",
    "    ax.plot(missing_genre_data)\n",
    "    ax.set(xlabel='Year', ylabel='Percentage')\n",
    "    ax.set_title(region)\n",
    "    x_values = ax.get_xticks()\n",
    "    new_x = [x_values[i] for i in [0, len(x_values) //2, -1]]\n",
    "    ax.set_xticks(new_x)\n",
    "    \n",
    "    #Compute overall percentage of missing data\n",
    "    tot_missing_data=sum(df[\"genre_is_na\"])*100/len(df)\n",
    "    return tot_missing_data\n",
    "\n",
    "fig, axs = plt.subplots(4, sharey=True, figsize=(15, 10), layout='constrained')\n",
    "fig.suptitle(\"Percentage of missing data per year\")\n",
    "prop_tot_asia = percentage_missing_data(asia_genres_df, \"Asia\", axs[0])\n",
    "prop_tot_europe = percentage_missing_data(europe_genres_df, \"Europe\", axs[1])\n",
    "prop_tot_india = percentage_missing_data(india_genres_df, \"India\", axs[2])\n",
    "prop_tot_northen_america = percentage_missing_data(northen_america_genres_df, \"Northen america\", axs[3])\n",
    "plt.show()\n",
    "\n",
    "print(\"Percentage of missing data for genre:\")\n",
    "print(\"- Asia: %.2f\" % prop_tot_asia)\n",
    "print(\"- Europe: %.2f\" % prop_tot_europe)\n",
    "print(\"- India: %.2f\" % prop_tot_india)\n",
    "print(\"- Northen America: %.2f\" % prop_tot_northen_america)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absence of data for movie genres is notably minimal, varying between 0.17% to 5.73%. Indian -the only area surpassing  1%- display a slightly higher absence at 5.73%, yet it remains within reasonable bounds. Moreover, the missing data appear evenly spread across the years. Consequently, we've chosen to eliminate these movies from the subsequent phase of genre analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of movies whose genre is not specified\n",
    "print(\"Removing films from which the genre is not specified:\")\n",
    "print(\"- Number of Asian movies before:\", len(asia_genres_df))\n",
    "asia_genres_df=asia_genres_df[~asia_genres_df[\"genre_is_na\"]]\n",
    "print(\"- Number of Asian movies after:\", len(asia_genres_df))\n",
    "\n",
    "print(\"- Number of European movies before:\", len(europe_genres_df))\n",
    "europe_genres_df=europe_genres_df[~europe_genres_df[\"genre_is_na\"]]\n",
    "print(\"- Number of European movies after:\", len(europe_genres_df))\n",
    "\n",
    "print(\"- Number of Indian movies before:\", len(india_genres_df))\n",
    "india_genres_df=india_genres_df[~india_genres_df[\"genre_is_na\"]]\n",
    "print(\"- Number of Indian movies after:\", len(india_genres_df))\n",
    "\n",
    "print(\"- Number of Northen American movies before:\", len(northen_america_genres_df))\n",
    "northen_america_genres_df=northen_america_genres_df[~northen_america_genres_df[\"genre_is_na\"]]\n",
    "print(\"- Number of Northen American movies after:\", len(northen_america_genres_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With missing data addressed, it's time to have an overview of diverse movie genres across various geographical areas! First, let's have a look at the number of genres in each area, and the quantity of movies belonging to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each region, group the movies by their genre, and count the number of movies per genre\n",
    "def group_by_genre(genre_df):\n",
    "    genre_count = genre_df.explode('genre')\n",
    "    genre_count = genre_df.groupby('genre').count()['id'].reset_index()\n",
    "    genre_count = genre_count.rename(columns={'id': 'movies_count'})\n",
    "    genre_count = genre_count.sort_values('movies_count', ascending=False)\n",
    "    display(genre_count.head(10))\n",
    "    return genre_count\n",
    "    \n",
    "asia_genre_count=group_by_genre(asia_genres_df)\n",
    "europe_genre_count=group_by_genre(europe_genres_df)\n",
    "india_genre_count=group_by_genre(india_genres_df)\n",
    "northen_america_genre_count=group_by_genre(northen_america_genres_df)\n",
    "\n",
    "#Print the number of different genres\n",
    "print(\"Number of genre categories for Asian film: \", len(asia_genre_count))\n",
    "print(\"Number of genre categories for European film: \", len(europe_genre_count))\n",
    "print(\"Number of genre categories for Indian film: \", len(india_genre_count))\n",
    "print(\"Number of genre categories for Northen American film: \", len(northen_america_genre_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's quite a variety of genres here! To streamline our analysis, we'll focus on the primary movie genres in each geographical region. We'll retain genres that appear in at least 2% of the films for this purpose, and group all the other in a category named \"Others\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Keep only genre that represents 2% of the movie, and group the other in a categorie name \"others\"\n",
    "def add_category_other(genre_count, threshold):\n",
    "    genre_count_df = genre_count[genre_count[\"movies_count\"]/genre_count[\"movies_count\"].sum()>threshold]\n",
    "    new_row = pd.DataFrame(data = {'genre': ['Others'], 'movies_count' : [genre_count[genre_count[\"movies_count\"]/genre_count[\"movies_count\"].sum()<=threshold].movies_count.sum()]})\n",
    "    genre_count_df = pd.concat([genre_count_df, new_row])\n",
    "    return genre_count_df\n",
    "\n",
    "asia_genre_count_df = add_category_other(asia_genre_count, 0.02)\n",
    "europe_genre_count_df = add_category_other(europe_genre_count, 0.02)\n",
    "india_genre_count_df = add_category_other(india_genre_count, 0.02)\n",
    "northen_america_genre_count_df = add_category_other(northen_america_genre_count, 0.02)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "# Dictionary to store genre-color mapping\n",
    "genre_colors = {}\n",
    "\n",
    "# Loop through each region and plot the pie chart with consistent colors\n",
    "for i, (region_df, title) in enumerate(zip([asia_genre_count_df, europe_genre_count_df, india_genre_count_df, northen_america_genre_count_df],['Asia', 'Europe', 'India', 'North America'])):\n",
    "    colors = []\n",
    "    for genre in region_df['genre']:\n",
    "        if genre not in genre_colors:  # If the genre doesn't have a color assigned yet, generate a random color and save it\n",
    "            genre_colors[genre] = sns.color_palette('hls', n_colors=len(region_df['genre']))[len(colors)]\n",
    "        colors.append(genre_colors[genre])  # Use the saved color for the genre\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax[row][col].pie(region_df.movies_count, labels=region_df.genre, autopct='%1.1f%%', colors=colors)\n",
    "    ax[row][col].set_title(title)\n",
    "fig.suptitle('Proportion of movie genre per region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe prevalent genres shared across geographical boundaries, like drama consistently securing a top-three position in every region. However, certain genres exhibit more regional specificity, such as martial arts in Asia or musicals in India. Investigating these similarities and differences will take center stage in the upcoming project milestone as we delve deeper into our analysis. Particularly intriguing is examining the primary genres shared among countries and exploring each genre's uniqueness regarding ethnicity and gender representation.\n",
    "\n",
    "It's essential to highlight that the \"other\" category regroup a significant portion of the movies. This arises from the high specificity of keywords used for genre classification, resulting in numerous, smaller genre categories, many of which contain a low number of movies. Thus, we should consider a potential grouping of related genres into broader categories. Hence, data analysis approach for genres during next milestone will likely involve employing clustering techniques, leveraging pre-trained word embeddings to combine genres into more encompassing categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gender representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exploration of ethnicity representation, we are going to use the file characters.csv to analysis the actors' ethnicity in movies produced in the different geograhic areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and display the head of the created Dataframe\n",
    "characters_df = pd.read_csv(CLEAN_DATA_PATH + 'characters.csv')\n",
    "display(characters_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of our project is to analyse the film industry across four key geographical regions. To do so, our initial step involves generating four \"sub-dataframes\" to examine character data, enabling us to explore ethnicities in diverse global regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movies data set in order to retreive ID-area corresponding\n",
    "movie_df = pd.read_csv(CLEAN_DATA_PATH + 'movies.csv')\n",
    "merged_df = pd.merge(characters_df, movie_df[['id', 'area']], left_on='movie_id', right_on='id', how='left', suffixes=('', '_drop'))\n",
    "merged_df = merged_df.drop('id_drop', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat four sub-dataframe extracted from character.csv for the four regions\n",
    "asia_characters_df = merged_df[merged_df['area'] == 'Asia']\n",
    "europe_characters_df = merged_df[merged_df['area'] == 'Europe']\n",
    "india_characters_df = merged_df[merged_df['area'] == 'India']\n",
    "northen_america_characters_df = merged_df[merged_df['area'] == 'Northen America']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing data in actor ethnicity**  \n",
    "The fisrt information needed for this exploration is the proportion of missing values for the actors' ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the proportion of missing ethnicity data\n",
    "def percentage_missing_data(df, region, ax):\n",
    "    # Indicator variable showing that something is NA\n",
    "    df.loc[:,[\"ethnicity_is_na\"]] = df.actor_ethinicity_id.isna()\n",
    "\n",
    "    # % of missing data\n",
    "    missing_ethnicity_data = df.groupby(\"movie_release_year\").ethnicity_is_na.mean()\n",
    "    \n",
    "    # Plot it\n",
    "    ax.plot(missing_ethnicity_data)\n",
    "    ax.set(xlabel='Year', ylabel='Percentage')\n",
    "    ax.set_title(region)\n",
    "    x_values = ax.get_xticks()\n",
    "    new_x = [x_values[i] for i in [0, len(x_values) //2, -1]]\n",
    "    ax.set_xticks(new_x)\n",
    "    \n",
    "    #return overall percentage of missing data\n",
    "    prop_missing_data=sum(df[\"ethnicity_is_na\"])*100/len(df)\n",
    "    return prop_missing_data\n",
    "\n",
    "fig, axs = plt.subplots(4, sharey=True, figsize=(15, 10), layout='constrained')\n",
    "fig.suptitle(\"Percentage of missing data per year\")\n",
    "prop_missing_asia=percentage_missing_data(asia_characters_df.copy(), \"Asia\", axs[0])\n",
    "prop_missing_europe=percentage_missing_data(europe_characters_df.copy(), \"Europe\", axs[1])\n",
    "prop_missing_india=percentage_missing_data(india_characters_df.copy(), \"India\", axs[2])\n",
    "prop_missing_northen_america=percentage_missing_data(northen_america_characters_df.copy(), \"Northen America\", axs[3])\n",
    "plt.show()\n",
    "\n",
    "# PrintOverall proportion of missing data\n",
    "print(\"Percentage of missing revenue data:\")\n",
    "print(\"- Asia: %.2f\" % prop_missing_asia)\n",
    "print(\"- Europe: %.2f\" % prop_missing_europe)\n",
    "print(\"- India: %.2f\" % prop_missing_india)\n",
    "print(\"- Northen America: %.2f\" % prop_missing_northen_america)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a significant portion of NaN values in actor ethnicity across our four distinct groups. We can notice that this percentage is slightly decreasing over the last decades, but it remains quit important and not negligeable in our final analysis conclusions. Due to resource constraints, we opt to remove these NaN values, as obtaining corresponding ethnicity for such a large number of actors through scraping is not feasible for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_asia_df_na = len(asia_characters_df)\n",
    "asia_characters_df = asia_characters_df.dropna(subset=['actor_ethinicity_id'])\n",
    "print(\"We removed \", len_asia_df_na - len(asia_characters_df), \" character of asian movies with no registered ethnicity ID\")\n",
    "print(\"It remains \", len(asia_characters_df), \" character of asian movies.\")\n",
    "\n",
    "len_europe_df_na = len(europe_characters_df)\n",
    "europe_characters_df = europe_characters_df.dropna(subset=['actor_ethinicity_id'])\n",
    "print(\"We removed \", len_europe_df_na - len(europe_characters_df), \" character of european movies with no registered ethnicity ID\")\n",
    "print(\"It remains \", len(europe_characters_df), \" character of european movies.\")\n",
    "\n",
    "len_india_df_na = len(india_characters_df)\n",
    "india_characters_df = india_characters_df.dropna(subset=['actor_ethinicity_id'])\n",
    "print(\"We removed \", len_india_df_na - len(india_characters_df), \" character of indian movies with no registered ethnicity ID\")\n",
    "print(\"It remains \", len(india_characters_df), \" character of indian movies.\")\n",
    "\n",
    "len_northen_america_df_na = len(northen_america_characters_df)\n",
    "northen_america_characters_df = northen_america_characters_df.dropna(subset=['actor_ethinicity_id'])\n",
    "print(\"We removed \", len_northen_america_df_na - len(northen_america_characters_df), \" character of northen american movies with no registered ethnicity ID\")\n",
    "print(\"It remains \", len(northen_america_characters_df), \" character of northen american movies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform author ethnicity ID**  \n",
    "The author ethnicity of each actor is registered in the dataset by a Freebase ID. In order to obtain the corresponding labels we retrieve information from the Wikidata knowledge base using a query language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ethnicity(freebase_id):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = \"\"\"\n",
    "    SELECT ?item ?itemLabel WHERE {\n",
    "      ?item wdt:P646 '\"\"\" + str(freebase_id) + \"\"\"'.\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            label = results[\"results\"][\"bindings\"][0][\"itemLabel\"][\"value\"]\n",
    "            return label\n",
    "        else:\n",
    "            return \"No label\"\n",
    "    except Exception as e:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the 4 groups remove all ethnicity ID duplicates to make the query fastest\n",
    "asia_df_unique_ethnicity = asia_characters_df.copy()\n",
    "asia_df_unique_ethnicity = asia_df_unique_ethnicity.drop_duplicates(subset='actor_ethinicity_id', keep='first')\n",
    "\n",
    "europe_df_unique_ethnicity = europe_characters_df.copy()\n",
    "europe_df_unique_ethnicity = europe_df_unique_ethnicity.drop_duplicates(subset='actor_ethinicity_id', keep='first')\n",
    "\n",
    "india_df_unique_ethnicity = india_characters_df.copy()\n",
    "india_df_unique_ethnicity = india_df_unique_ethnicity.drop_duplicates(subset='actor_ethinicity_id', keep='first')\n",
    "\n",
    "northen_america_df_unique_ethnicity = northen_america_characters_df.copy()\n",
    "northen_america_df_unique_ethnicity = northen_america_df_unique_ethnicity.drop_duplicates(subset='actor_ethinicity_id', keep='first')\n",
    "\n",
    "# Creat a Data Frame with only unique ethnicity present in our four groups of interest\n",
    "df_unique_ethnicity = pd.concat([asia_df_unique_ethnicity, europe_df_unique_ethnicity, india_df_unique_ethnicity, northen_america_df_unique_ethnicity], ignore_index=True)\n",
    "df_unique_ethnicity = df_unique_ethnicity.drop_duplicates(subset='actor_ethinicity_id', keep='first')\n",
    "df_unique_ethnicity = df_unique_ethnicity[['actor_ethinicity_id']]\n",
    "\n",
    "print(\"Number of different actor ethnicithies represented in all our movies of interest is: \", len(df_unique_ethnicity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ethnicity labels corresponding to the different IDs thanks to Wikidata\n",
    "df_unique_ethnicity['ethnicity_Label'] = df_unique_ethnicity['actor_ethinicity_id'].apply(get_ethnicity)\n",
    "\n",
    "# Save the ethnicity labels / IDs corresponding data into a CSV file\n",
    "df_unique_ethnicity.to_csv(CLEAN_DATA_PATH + 'etnicity_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file with the ethnicity ID-label mapping\n",
    "ethnicity_mapping = pd.read_csv(CLEAN_DATA_PATH + 'etnicity_labels.csv')\n",
    "\n",
    "# Add a new column of our character Data Frame with a new column cprresponding to ethnicity label\n",
    "asia_characters_df = pd.merge(asia_characters_df, ethnicity_mapping, how='left', on='actor_ethinicity_id')\n",
    "europe_characters_df = pd.merge(europe_characters_df, ethnicity_mapping, how='left', on='actor_ethinicity_id')\n",
    "india_characters_df = pd.merge(india_characters_df, ethnicity_mapping, how='left', on='actor_ethinicity_id')\n",
    "northen_america_characters_df = pd.merge(northen_america_characters_df, ethnicity_mapping, how='left', on='actor_ethinicity_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain ethnicity IDs are absent from the Wikidata database, making it impossible to retrieve their corresponding labels. In our function, these ethnicity IDs are linked to the label \"No label.\" We decided to eliminate these labels and treat them as missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove values with unknow ethnicity labels\n",
    "asia_characters_df = asia_characters_df[asia_characters_df['ethnicity_Label'] != \"No label\"]\n",
    "europe_characters_df = europe_characters_df[europe_characters_df['ethnicity_Label'] != \"No label\"]\n",
    "india_characters_df = india_characters_df[india_characters_df['ethnicity_Label'] != \"No label\"]\n",
    "northen_america_characters_df = northen_america_characters_df[northen_america_characters_df['ethnicity_Label'] != \"No label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ethnic diversity across our different regions**  \n",
    "To get a better idea of which ethnicities are represented in our four different groups and to identify the predominant one we decided to visualize this information through four separate word clouds. In these word clouds, the size of each ethnicity name is proportional to its frequency of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all different ethnicities present in Asian films into a list\n",
    "word_list_asia = asia_characters_df['ethnicity_Label'].tolist()\n",
    "word_list_europe = europe_characters_df['ethnicity_Label'].tolist()\n",
    "word_list_india = india_characters_df['ethnicity_Label'].tolist()\n",
    "word_list_northen_america = northen_america_characters_df['ethnicity_Label'].tolist()\n",
    "\n",
    "# Group lists to be able to iterate over a loop\n",
    "word_lists = [word_list_asia, word_list_europe, word_list_india, word_list_northen_america]\n",
    "# Group list to print word cloud title\n",
    "region_str_list = [\"Asian movies actor ethnicities\",\n",
    "                   \"European movies actor ethnicities\",\n",
    "                   \"Indian movies actor ethnicities\",\n",
    "                   \"Northen american movies actor ethnicities\"]\n",
    "colormaps = ['Oranges', 'Blues', 'PuRd_r', 'YlGn_r']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "# Iterate through the word lists and titles\n",
    "for i, region_list in enumerate(word_lists):\n",
    "    # Convert float values to strings\n",
    "    word_list = [str(word) for word in region_list if not isinstance(word, float)] \n",
    "    # Count occurrences for the different ethnicities\n",
    "    word_counts = Counter(word_list) \n",
    "    # Generate the word cloud with size proportional to occurrences\n",
    "    wordcloud = WordCloud(width=400, height=200, background_color='white', colormap=colormaps[i]).generate_from_frequencies(word_counts)  \n",
    "    # Plot the word cloud on the corresponding subplot\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis('off')   \n",
    "    # Set the title for the subplot\n",
    "    ax.set_title(region_str_list[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the ethnicities linked to actors in this CMU movie dataset are highly specific, making it challenging to form broader subgroups that ethically group different ethnicities together. Indeed we will have to face two major problems for a deeper an more genral exploration of ethicity representation:  \n",
    "- Presence of two many precise categories ungroupable\n",
    "- Occurence of many ethnicithies with a very small frequencies which are part of the dataset and we have to use them to avoid biases and misrepresentation in the results  \n",
    "\n",
    "Film industries in our four region of the world are large melting pot of actors coming from different horizon. In order to countinue our exploration we are going to focus on the raw count of different ethnicities present in the different \"film regions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a list grouping ouf four datasets\n",
    "dataframes = [asia_characters_df, europe_characters_df, india_characters_df, northen_america_characters_df]\n",
    "# Get unique ethnicities for each group\n",
    "unique_ethnicities = [df['ethnicity_Label'].unique() for df in dataframes]\n",
    "# Count the number of unique ethnicities for each group\n",
    "counts_per_group = [len(ethni) for ethni in unique_ethnicities]\n",
    "# Create a DataFrame with counts and names\n",
    "df_counts = pd.DataFrame({'Counts': counts_per_group, 'Names': [\"Asia\", \"Europe\", \"India\", \"Northen American\"]})\n",
    "# Sort the DataFrame by counts in descending order\n",
    "df_counts = df_counts.sort_values(by='Counts', ascending=False)\n",
    "\n",
    "# Plots the different counts\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plots the different counts\n",
    "ax.bar(range(len(df_counts)), df_counts['Counts'], align='center', alpha=0.7)\n",
    "\n",
    "# Set labels, axis and title\n",
    "ax.set_xlabel('Regions of the world')\n",
    "ax.set_ylabel('Count of unique ethnicities (all years combined)')\n",
    "ax.set_title('Count of unique ethnicities across the geographical regions')\n",
    "ax.set_xticks(range(len(df_counts)))\n",
    "ax.set_xticklabels(df_counts['Names'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only look a raw countings of the different ethnicities present in the four different geographical groups over all years, we can clearly see that more different ethnicies are represented in Northern American and Western European movies compared to Asian and Indian filmss.  \n",
    "\n",
    "However, caution is necessary when interpreting this plot, as it represents raw counts without considering the total number of movies produced by each industry. It is obvious that an industry which produces a huge amount of films provides more opportunities for ethnic representation.  \n",
    "\n",
    "In order to go further in our exploration we are going to compare how this ethnicity diversity evolves across years in our four group when taking into account the amout of produced movies. The large proportion of missing values, which is unevenly distributed among the groups, makes these plots less reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, raw_count):\n",
    "    df['movie_release_date'] = pd.to_numeric(df['movie_release_date'], errors='coerce')\n",
    "    df = df[df['movie_release_date'].between(1970, 2013)]\n",
    "    \n",
    "    # Group by 'movie_release_date' and calculate the number of rows for each year\n",
    "    year_counts = df.groupby('movie_release_date').size()\n",
    "    \n",
    "    # Group by 'movie_release_date' and calculate the weighted count of unique ethnicities\n",
    "    unique_ethnicities = df.groupby('movie_release_date')['ethnicity_Label'].nunique()\n",
    "\n",
    "    if not(raw_count):\n",
    "        # Ensure the correct alignment of counts with the corresponding years\n",
    "        weighted_ethnicities = weighted_ethnicities = pd.Series((unique_ethnicities.values / year_counts.values)*100,\n",
    "                                          index=unique_ethnicities.index)\n",
    "    elif raw_count:\n",
    "        weighted_ethnicities = unique_ethnicities\n",
    "    \n",
    "    return weighted_ethnicities, year_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each DataFrame\n",
    "unique_asian_ethnicities, asian_count  = preprocess_df(asia_characters_df, False)\n",
    "unique_european_ethnicities, european_count = preprocess_df(europe_characters_df, False)\n",
    "unique_indian_ethnicities, indian_count = preprocess_df(india_characters_df, False)\n",
    "unique_na_ethnicities, na_count = preprocess_df(northen_america_characters_df, False)\n",
    "\n",
    "unique_asian_ethnicities_count, asian_count  = preprocess_df(asia_characters_df, True)\n",
    "unique_european_ethnicities_count, european_count = preprocess_df(europe_characters_df, True)\n",
    "unique_indian_ethnicities_count, indian_count = preprocess_df(india_characters_df, True)\n",
    "unique_na_ethnicities_count, na_count = preprocess_df(northen_america_characters_df, True)\n",
    "\n",
    "# Create a common date range\n",
    "year_range = range(1942, 2014)\n",
    "\n",
    "# Reindex each DataFrame to fill NaN for missing years\n",
    "unique_asian_ethnicities = unique_asian_ethnicities.reindex(year_range)\n",
    "unique_european_ethnicities = unique_european_ethnicities.reindex(year_range)\n",
    "unique_indian_ethnicities = unique_indian_ethnicities.reindex(year_range)\n",
    "unique_na_ethnicities = unique_na_ethnicities.reindex(year_range)\n",
    "\n",
    "unique_asian_ethnicities_count = unique_asian_ethnicities_count.reindex(year_range)\n",
    "unique_european_ethnicities_count = unique_european_ethnicities_count.reindex(year_range)\n",
    "unique_indian_ethnicities_count = unique_indian_ethnicities_count.reindex(year_range)\n",
    "unique_na_ethnicities_count = unique_na_ethnicities_count.reindex(year_range)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plotting\n",
    "ax1.plot(asian_count.index, asian_count.values, marker='o', linestyle='-', color='tab:orange', markersize=3, label='asian movies')\n",
    "ax1.plot(european_count.index, european_count.values, marker='o', linestyle='-', color='tab:blue', markersize=3, label='european movies')\n",
    "ax1.plot(indian_count.index, indian_count.values, marker='o', linestyle='-', color='tab:pink', markersize=3, label='indian movies')\n",
    "ax1.plot(na_count.index, na_count.values, marker='o', linestyle='-', color='tab:olive', markersize=3, label='northen american movies')\n",
    "# Set labels and title\n",
    "ax1.set_xlabel('Year of movie release')\n",
    "ax1.set_ylabel('Count of released movies')\n",
    "ax1.set_title('Count of released movies over the years')\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax1.grid()\n",
    "\n",
    "# Plotting\n",
    "ax2.plot(unique_asian_ethnicities_count.index, unique_asian_ethnicities_count.values, marker='o', linestyle='-', color='tab:orange', markersize=3, label='asian movies')\n",
    "ax2.plot(unique_asian_ethnicities_count.index, unique_european_ethnicities_count.values, marker='o', linestyle='-', color='tab:blue', markersize=3, label='european movies')\n",
    "ax2.plot(unique_asian_ethnicities_count.index, unique_indian_ethnicities_count.values, marker='o', linestyle='-', color='tab:pink', markersize=3, label='indian movies')\n",
    "ax2.plot(unique_asian_ethnicities_count.index, unique_na_ethnicities_count.values, marker='o', linestyle='-', color='tab:olive', markersize=3, label='northen american movies')\n",
    "# Set labels and title\n",
    "ax2.set_xlabel('Year of movie release')\n",
    "ax2.set_ylabel('Raw count different actor ethnicities')\n",
    "ax2.set_title('Raw count of different ethnicities over the years')\n",
    "ax2.legend(loc=\"upper left\")\n",
    "ax2.grid()\n",
    "\n",
    "# Plotting\n",
    "ax3.plot(unique_asian_ethnicities.index, unique_asian_ethnicities.values, marker='o', linestyle='-', color='tab:orange', markersize=3, label='asian movies')\n",
    "ax3.plot(unique_asian_ethnicities.index, unique_european_ethnicities.values, marker='o', linestyle='-', color='tab:blue', markersize=3, label='european movies')\n",
    "ax3.plot(unique_asian_ethnicities.index, unique_indian_ethnicities.values, marker='o', linestyle='-', color='tab:pink', markersize=3, label='indian movies')\n",
    "ax3.plot(unique_asian_ethnicities.index, unique_na_ethnicities.values, marker='o', linestyle='-', color='tab:olive', markersize=3, label='northen american movies')\n",
    "# Set labels and title\n",
    "ax3.set_xlabel('Year of movie release')\n",
    "ax3.set_ylabel('Percentage of different actor ethnicities (%)')\n",
    "ax3.set_title('Percentage of different ethnicities over the years')\n",
    "ax3.legend(loc=\"upper left\")\n",
    "ax3.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify our interpretation, we decided to focus our attention on films released after 1970 due to a significant proportion of missing values and a limited number of available films before this date.\n",
    "\n",
    "In the initial plot, we observe the film production trends across our four geographical regions:  \n",
    "- The number of eastern asian films remains consistently low and quite constant over the years. This will make the ethnicity diversity proportional analysis more complicated for this region.\n",
    "- Since the 1980s, film production has increased in the other three regions, offering more opportunities for ethnic representation diversification.\n",
    "- European and Indian curves appear almost superimposed, facilitating a more straightforward comparison of ethnic representation between these two regions.\n",
    "- The Northern American film industry appears significantly larger than the other three, contributing to the higher raw count in the previous bar plot. Even if the overall shape is quite similar to the one of European and Indian movies, the counting values are largely superior. \n",
    "- Last few years are associated with a drastic reduced amount of film available in our data set making comparisons difficult since 2010.\n",
    "\n",
    "In the second plot, we observe the raw count of different ethnicities represented over the years:   \n",
    "- The ethnicity count number increased the most in Northern American and European films.\n",
    "- In Eastern Asian and Indian films we can also observed a raw count increase even if the different across years is smaller.\n",
    "\n",
    "The third plot illustrates the percentage of different ethnicities represented in each region for different years. To achieve this, we divided the ethnicity count by the total count of films in a specific region and year:  \n",
    "- Values for Eastern Asian unique ethnicities percentage are really non readable and comparable as we have a very few movies available. We observe a high variability of our curve over years.  \n",
    "- Globaly the other three curves show a slight decreasing pattern. This suggests that, despite an increase in the number of films produced, ethnic diversity representation did not rise sufficiently to maintain a constant percentage or show an increase. Indeed if we compare the slope of curves of the first plot and the ones for the second (assuming the same y-axis for both), the second plot's slopes are notably smaller. This implies that the number of movies produced increased more rapidly than the diversification of ethnicities represented on screen.\n",
    "- As we said before european and asian curve can be easily comparable as their number of produced films are quite similar. On this second plot, these two curves evolves in parallel, but the percentage of different ethnicities in  european movies remains larger than the one for asian movies. This indicates that, for an equivalent number of produced films, ethnic diversity is more significant in European-produced movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Exploration of plot summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will first analyze the results of the CoreNLP pipeline already run on the summaries, stored in ```clean_data/NLP_summaries```, and then apply sentiment analysis on the raw summaries\n",
    "\n",
    "**CoreNLP results for one summary**\n",
    "\n",
    "To begin, we will select one movie by specifying its ID and look at the data from the corresponding XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new data folder\n",
    "NLP_SUMMARIES = 'clean_data/NLP_summaries/'\n",
    "\n",
    "# Select a movie\n",
    "movie_id = 3217\n",
    "\n",
    "# Path to the XML file\n",
    "summary_test_file = NLP_SUMMARIES + str(movie_id) + '.xml'\n",
    "print(summary_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `<token>` element represents a word from a sentence, identified by a unique `id`. Inside each `<token>` element, there are several child elements that provide more information about the token:\n",
    "\n",
    "- `<word>`: The actual word in the text.\n",
    "- `<lemma>`: The base or dictionary form of the word.\n",
    "- `<CharacterOffsetBegin>` and `<CharacterOffsetEnd>`: The start and end positions of the word in the original text.\n",
    "- `<POS>`: The part-of-speech tag for the word. POS tagging is the task of labeling the words in a sentence with their appropriate part of speech (noun, verb, adjective, etc.).\n",
    "- `<NER>`: Named Entity Recognition tag. NER is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "The `<sentence>` element contains a set of these tokens, representing a sentence in the text. The `id` attribute of the `<sentence>` element indicates the order of the sentence in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the XML file and get the root element\n",
    "tree = ET.parse(summary_test_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Create an empty dict to store data\n",
    "tokens_dict = {}\n",
    "\n",
    "# Iterate over all 'token' elements in the document\n",
    "for token in root.iter('token'):\n",
    "    # Extract data from the child elements of 'token'\n",
    "    word = token.find('word').text\n",
    "    lemma = token.find('lemma').text\n",
    "    pos = token.find('POS').text\n",
    "    ner = token.find('NER').text\n",
    "    \n",
    "    # Store in dict\n",
    "    token_id = token.attrib['id']\n",
    "    tokens_dict[token_id] = {\n",
    "        'word': word,\n",
    "        'lemma': lemma,\n",
    "        'POS': pos,\n",
    "        'NER': ner,\n",
    "    }\n",
    "\n",
    "    # Print the extracted data\n",
    "    #print(f'Word: {word}, Lemma: {lemma}, POS: {pos}, NER: {ner}')\n",
    "\n",
    "# Print the dict\n",
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence also has dependency informations between tokens:\n",
    "\n",
    "- `<basic-dependencies>`: These represent grammatical relationships between words in a sentence. For example, the dependencies between noun and verb.\n",
    "\n",
    "- `<collapsed-dependencies>`: These are a simplified form of dependencies where certain types of indirect dependencies are collapsed into direct dependencies for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dict to store data\n",
    "dep_dict = {}\n",
    "\n",
    "# Extract and print dependencies\n",
    "for i,dep in enumerate(root.iter('dep')):\n",
    "    # Extract data\n",
    "    dep_type = dep.attrib['type']\n",
    "    governor = dep.find('governor').text\n",
    "    dependent = dep.find('dependent').text\n",
    "\n",
    "    # Store in dict\n",
    "    dep_dict[i] = {\n",
    "        'type': dep_type,\n",
    "        'governor': governor,\n",
    "        'dependent': dependent\n",
    "    }\n",
    "    \n",
    "    # Print the extracted data\n",
    "    # print(f\"Dependency type: {dep_type}, Governor: {governor}, Dependent: {dependent}\")\n",
    "\n",
    "# Print the dict\n",
    "print(dep_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, dependencies between different sentences can be analyzed:\n",
    "\n",
    "- `<coreferences>`: These represent instances where multiple expressions in a text refer to the same entity. \n",
    "\n",
    "In the XML, each `<coreference>` element represents a group of mentions that all refer to the same entity in the text. Each `<mention>` element within a `<coreference>` represents a specific instance where that entity is mentioned in the text.\n",
    "The `<mention representative=\"true\">` is the primary mention in the text that other mentions refer back to. \n",
    "\n",
    "The `<sentence>` tag within a `<mention>` indicates the sentence number where the mention occurs. The `<start>` and `<end>` tags indicate the position of the start and end of the mention within that sentence. The `<head>` tag indicates the head word of the mention.\n",
    "\n",
    "To know what a coreference corresponds to in the actual text, we would need to find the sentences and word positions indicated by the `<sentence>`, `<start>`, and `<end>` tags in the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dict to store data\n",
    "coref_dict = {}\n",
    "\n",
    "# Extract and print coreferences\n",
    "for i,coref in enumerate(root.iter('coreference')):\n",
    "    coref_dict[i] = {}\n",
    "    for j,mention in enumerate(coref.iter('mention')):\n",
    "        # Extract data\n",
    "        sentence = mention.find('sentence').text\n",
    "        start = mention.find('start').text\n",
    "        end = mention.find('end').text\n",
    "\n",
    "        # Store in dict\n",
    "        coref_dict[i][j] = {\n",
    "            'sentence': sentence,\n",
    "            'start': start,\n",
    "            'end': end\n",
    "        }\n",
    "\n",
    "        # Print the extracted data\n",
    "        #print(f\"Sentence: {sentence}, Start: {start}, End: {end}\")\n",
    "\n",
    "# Print the dict\n",
    "print(coref_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally get for each summary:\n",
    "- A `token dictionnary` containing the word, lemma, POS and NER of each tokens (words) of the summary.\n",
    "- A `dependency dictionnary` containing each gramatical dependencies found in the summary along with the dependency type.\n",
    "- A `coreference dictionnary` referencing for each coreference (entity refered to) all the group of words related to it in a specific sentence.\n",
    "\n",
    "The nexts steps of this analysis will consist in extracting such data for all the summaries and include the relevant informations for the different analysis axes above. This will be done for P3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis for a few summaries**\n",
    "\n",
    "In this part, we will apply sentiment analysis on the sentences of one summary. To do this, we use the stanza package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the raw summaries from the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "DATA_PATH = 'clean_data/'\n",
    "\n",
    "# Load summaries\n",
    "summaries_df = pd.read_csv(DATA_PATH + 'movies_summaries.csv')\n",
    "\n",
    "# Display dataframe\n",
    "display(summaries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will import the stanza model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download english model\n",
    "stanza.download('en')\n",
    "\n",
    "# initialize English neural pipeline\n",
    "nlp = stanza.Pipeline(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to convert the summaries to stanza documents to be able to use the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the summaries column of df into a list\n",
    "summaries = summaries_df[\"summary\"].tolist()\n",
    "\n",
    "# Select a subset of summaries for testing\n",
    "summaries_test = summaries[0:4]\n",
    "\n",
    "# Wrap each document with a stanza.Document object\n",
    "summaries_list = [stanza.Document([], text=p) for p in summaries_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now iterate on the test list to get the sentiment scores. The sentiment is expressed by a simple classification: \n",
    "- 1 = Negative sentiment\n",
    "- 2 = Neutral sentiment\n",
    "- 3 = Positive sentiment\n",
    "\n",
    "Each sentence of the summaries is associated with such class, resulting in a list of sentiment scores for each summary.\n",
    "\n",
    "We iterate on a loop instead of passing the list directly to the function to avoid running out of RAM on such large data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,summary in enumerate(summaries_list):\n",
    "  print(\"---------\")\n",
    "  print(f\"Summary {i+1}:\")\n",
    "  # Print the summary\n",
    "  print(summaries_test[i])\n",
    "\n",
    "  # Run nlp pipeline\n",
    "  doc = nlp(summary)\n",
    "\n",
    "  # Print sentences sentiments\n",
    "  sentiment_scores = []\n",
    "  for i, sentence in enumerate(doc.sentences):\n",
    "    sentiment_scores.append(sentence.sentiment)\n",
    "  print(\"Sentiment scores: \", sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus get for each summary a list of sentiment scores corresponding to each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
